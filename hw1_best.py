# -*- coding: utf-8 -*-
"""hw1_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TvbNcKQWtlttuSLfPHAkN-mEqvy7wv7T

# **Goal & Download**

本次目標：由前 9 個小時的 18 個 features (包含 PM2.5)預測的 10 個小時的 PM2.5。可以參考 https://docs.google.com/presentation/d/18MG1wSTTx8AentGnMfIRUp8ipo8bLpgAj16bJoqW-b0/edit#slide=id.g4cd6560e29_0_10 獲知更細項的作業說明。

<!-- 首先，從 https://drive.google.com/open?id=1El0zvTkrSuqCTDcMpijXpADvJzZC2Jpa 將整個資料夾下載下來，並將下載下來的資料夾放到自己的 Google Drive（注意：上傳到自己 Google Drive 的是資料夾 hw1-regression，而非壓縮檔） -->


若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com

# **Load 'train.csv'**
train.csv的資料為12個月中，每個月取20天，每天24小時的資料(每小時資料有18個features)。
"""

import sys
import pandas as pd
import numpy as np
from sys import argv
#from google.colab import drive 
#!gdown --id '1wNKAxQ29G15kgpBy_asjTcZRRgmsCZRm' --output data.zip
#!unzip data.zip
# data = pd.read_csv('gdrive/My Drive/hw1-regression/train.csv', header = None, encoding = 'big5')
data_path_t = argv[1]

data = pd.read_csv(data_path_t, encoding = 'big5')
#print(data)

"""# **Preprocess (1)** 
取需要的數值部分，將 'RAINFALL' 欄位全部補 0。另外，如果要在 colab 重覆這段程式碼的執行，請從頭開始執行(把上面的都重新跑一次)，以避免跑出不是自己要的結果（若自己寫程式不會遇到，但 colab 重複跑這段會一直往下取資料。意即第一次取原本資料的第三欄之後的資料，第二次取第一次取的資料掉三欄之後的資料，...）。
"""

data = data.iloc[:, 3:]
data[data == 'NR'] = 0
raw_data = data.to_numpy()
raw_data

"""# **Extract Features (1)**
![圖片說明](https://drive.google.com/uc?id=1LyaqD4ojX07oe5oDzPO99l9ts5NRyArH)
![圖片說明](https://drive.google.com/uc?id=1ZroBarcnlsr85gibeqEF-MtY13xJTG47)

將原始 4320 * 18 的資料依照每個月分重組成 12 個 18 (features) * 480 (hours) 的資料。
"""

feature_set = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]
feature = len(feature_set)
pm2_5 = feature_set.index(9)
WD_HR = feature_set.index(14)
WIND_DIREC = feature_set.index(15)
double = feature_set.index(10)
degree = 345
#print(pm2_5) 
month_data = {}
for month in range(12):
    sample = np.empty([18, 480])
    sample_picked = np.empty([feature,480])
    for day in range(20):
        sample[:, day * 24 : (day + 1) * 24] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1), :]
    sample_picked = sample[feature_set,:]
    sample_picked[WD_HR,:] = np.cos((sample_picked[WD_HR,:]-degree)*np.pi/180)
    sample_picked[double,:] = np.cos((sample_picked[WD_HR,:]-degree)*np.pi/180)**2
    sample_picked[WIND_DIREC,:] = np.cos((sample_picked[WIND_DIREC,:]-degree)*np.pi/180)
    sample_picked[16,:] = np.cos((sample_picked[WIND_DIREC,:]-degree)*np.pi/180)**2
    #print(sample_picked[WD_HR,:])
    month_data[month] = sample_picked

"""# **Extract Features (2)**
![alt text](https://drive.google.com/uc?id=1wKoPuaRHoX682LMiBgIoOP4PDyNKsJLK)
![alt text](https://drive.google.com/uc?id=1FRWWiXQ-Qh0i9tyx0LiugHYF_xDdkhLN)

每個月會有 480hrs，每 9 小時形成一個 data，每個月會有 471 個 data，故總資料數為 471 * 12 筆，而每筆 data 有 9 * 18 的 features (一小時 18 個 features * 9 小時)。

對應的 target 則有 471 * 12 個(第 10 個小時的 PM2.5)
"""
x = np.empty([12 * 471, feature * 9], dtype = float)
y = np.empty([12 * 471, 1], dtype = float)
for month in range(12):
    for day in range(20):
        for hour in range(24):
            if day == 19 and hour > 14:
                continue            
            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)
            y[month * 471 + day * 24 + hour, 0] = month_data[month][pm2_5, day * 24 + hour + 9] #value...(7 -> pm2.5)
print(x)
print(y)

"""# **Normalize (1)**"""


mean_x = np.mean(x, axis = 0) #feat * 9 
std_x = np.std(x, axis = 0) #feat * 9 
for i in range(len(x)): #12 * 471
    for j in range(len(x[0])): #feat * 9 
        if std_x[j] != 0:
            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]
np.save('mean_x.npy', mean_x)
np.save('std_x.npy', std_x)
"""#**Split Training Data Into "train_set" and "validation_set"**
這部分是針對作業中report的第二題、第三題做的簡單示範，以生成比較中用來訓練的train_set和不會被放入訓練、只是用來驗證的validation_set。
"""

# import math
# x_train_set = x[: math.floor(len(x) * 0.8), :]
# y_train_set = y[: math.floor(len(y) * 0.8), :]
# x_validation = x[math.floor(len(x) * 0.8): , :]
# y_validation = y[math.floor(len(y) * 0.8): , :]
# print(x_train_set)
# print(y_train_set)
# print(x_validation)
# print(y_validation)
# print(len(x_train_set))
# print(len(y_train_set))
# print(len(x_validation))
# print(len(y_validation))

"""# **Training**
![alt text](https://drive.google.com/uc?id=1xIXvqZ4EGgmxrp7c9r0LOVbcvd4d9H4N)
![alt text](https://drive.google.com/uc?id=1S42g06ON5oJlV2f9RukxawjbE4NpsaB6)
![alt text](https://drive.google.com/uc?id=1BbXu-oPB9EZBHDQ12YCkYqtyAIil3bGj)

(和上圖不同處: 下面的code採用 Root Mean Square Error)

因為常數項的存在，所以 dimension (dim) 需要多加一欄；eps 項是避免 adagrad 的分母為 0 而加的極小數值。

每一個 dimension (dim) 會對應到各自的 gradient, weight (w)，透過一次次的 iteration (iter_time) 學習。
"""

dim = feature * 9 + 1
w = np.zeros([dim, 1])
x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)
learning_rate = 35
iter_time = 8000
adagrad = np.zeros([dim, 1])
eps = 0.0000000001
for t in range(iter_time):
    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12)#rmse
    if(t%100==0):
        print(str(t) + ":" + str(loss))
    gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) #dim*1
    adagrad += gradient ** 2
    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)
np.save('weight.npy', w)
w

